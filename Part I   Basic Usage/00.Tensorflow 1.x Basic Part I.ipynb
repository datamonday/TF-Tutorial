{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 两种在tensorflow 2.x版本运行1.x代码的方法\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "普通加法运算的结果：\n",
      " 5\n",
      "Tensorflow加法运算结果：\n",
      " Tensor(\"add:0\", shape=(), dtype=int32)\n",
      "c_t_value：\n",
      " 5\n"
     ]
    }
   ],
   "source": [
    "def tensorflow_demo():\n",
    "    '''\n",
    "    Tensorflow 基本结构\n",
    "    '''\n",
    "    # 原生Python的加法运算\n",
    "    a,b = 2,3\n",
    "    print(\"普通加法运算的结果：\\n\",a+b)\n",
    "    \n",
    "    # tensorflow实现加法运算\n",
    "    # 构建图\n",
    "    a_t,b_t = tf.constant(2),tf.constant(3)\n",
    "    print(\"Tensorflow加法运算结果：\\n\",a_t+b_t)\n",
    "    \n",
    "    # 开启会话\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        print(\"c_t_value：\\n\",sess.run(a_t+b_t))\n",
    "    \n",
    "    return None\n",
    "\n",
    "tensorflow_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.tensorflow 结构\n",
    "tensorflow 程序常被组织成一个构图阶段和一个执行图阶段。\n",
    "在构建阶段，数据与操作的执行被描述成一个图。\n",
    "    流程图：定义数据结构（张量tensor）和操作（节点operation）\n",
    "在执行阶段，使用会话执行构建好的图中的操作。\n",
    "    调用各方资源，将定义好的数据和操作运行起来。\n",
    "\n",
    "- 图和会话：\n",
    "    - 图：tensorflow将计算表示为指令之间的依赖关系；\n",
    "    - 会话：tensorflow跨一个或多个本地或远程设备运行数据流图的机制。\n",
    "- 张量：tensorflow中的基本数据对象\n",
    "- 节点：提供图中执行的操作。在数据流图中，节点通常以圆、椭圆或方框表示，代表对数据的运算或某种操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 数据流图\n",
    "构建数据流图时，需要两个基础元素：点（node）和边（edge）。\n",
    "- 节点：在数据流图中，节点通常以圆、椭圆或方框表示，代表对数据的运算或某种操作。例如，在图11-26中，就有5个节点，分别表示输入（input）、乘法（mul）和加法（add）。\n",
    "- 边：数据流图是一种有向图，“边”通常用带箭头线段表示，实际上，它是节点之间的连接。指向节点的边表示输入，从节点引出的边表示输出。输入可以是来自其他数据流图，也可以表示文件读取、用户输入。输出就是某个节点的“操作（Operation，下文简称Op）”结果。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置日志等级以屏蔽警告信息\n",
    "#import os\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL']='2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "log信息共有四个等级，按重要性递增为：\n",
    "INFO（通知）<WARNING（警告）<ERROR（错误）<FATAL（致命）;\n",
    "\n",
    "2、值的含义：不同值设置的是基础log信息（base_loging），运行时会输出base等级及其之上（更为严重）的信息。具体如下：\n",
    "\n",
    "||base_loging\t|屏蔽信息\t|输出信息|\n",
    "|--|--|--|--|\n",
    "|“0”\t|INFO\t|无|\tINFO + WARNING + ERROR + FATAL|\n",
    "|“1”\t|WARNING\t|INFO|\tWARNING + ERROR + FATAL|\n",
    "|“2”\t|ERROR\t|INFO + WARNING|\tERROR + FATAL|\n",
    "|“3”\t|FATAL\t|INFO + WARNING + ERROR|\tFATAL|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 图与tensorboard\n",
    "##### 2.2.1 什么是图结构？\n",
    "图结构：数据（tensor）+操作（operation）\n",
    "\n",
    "##### 2.2.2 图的相关操作\n",
    "    - 1.默认图\n",
    "        查看默认图的方法：\n",
    "            1）调用方法\n",
    "                tf.compat.v1.get_default_graph()\n",
    "            2）查看属性\n",
    "                .graph\n",
    "    - 2.创建图\n",
    "        使用tf.Graph()方法创建自定义图\n",
    "        new_g = tf.Graph()\n",
    "        with new_g.as_default():\n",
    "            定义数据和操作\n",
    "        with tf.Session(graph=new_g) as new_sess:\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_demo():\n",
    "    '''\n",
    "    图的演示\n",
    "    :return:\n",
    "    '''\n",
    "    # tensorflow实现加法运算\n",
    "    # 构建图\n",
    "    a_t,b_t = tf.constant(2),tf.constant(3)\n",
    "    c_t = a_t+b_t\n",
    "    \n",
    "    # a_t,b_t打印输出:\n",
    "    '''\n",
    "    a_t:\n",
    "     Tensor(\"Const_26:0\", shape=(), dtype=int32)\n",
    "    b_t:\n",
    "     Tensor(\"Const_27:0\", shape=(), dtype=int32)\n",
    "    '''\n",
    "    # 其中Const_26为指令名称，与tensorboard中graph显示的一致；\n",
    "    # 操作函数constant在运行过程中会生成一个操作对象operation，z冒号后边的“0”表示constant操作对象operation的输出个数为1个。\n",
    "    # 如果是1，则表示2个，输出索引从0开始。\n",
    "    print(\"a_t:\\n\",a_t)\n",
    "    print(\"b_t:\\n\",b_t)\n",
    "    print(\"c_t：\\n\",c_t )\n",
    "    \n",
    "    # 查看默认图\n",
    "    # 方法一：调用方法\n",
    "    default_g = tf.compat.v1.get_default_graph()\n",
    "    print('a_t的图属性：\\n',default_g)\n",
    "    \n",
    "    # 方法二：查看属性\n",
    "    print('a_t的图属性：\\n',a_t.graph)\n",
    "    \n",
    "    # 开启默认会话，tf.compat.v1.Session()运行默认图中的操作\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        print(\"c_t_value：\\n\",sess.run(c_t)) \n",
    "        # 也可用eval()方法，但必须在session中。多用在交互模式下。\n",
    "        # print(\"c_t_value：\\n\",c_t.eval()) \n",
    "        print('sess的图属性：\\n',sess.graph)\n",
    "        \n",
    "        # tensorboard\n",
    "        tf.compat.v1.summary.FileWriter('D:/AliyunEDU/04 summary/',graph=sess.graph)\n",
    "    \n",
    "    #-------------------------------------------------------\n",
    "    # 自定义图\n",
    "    new_g = tf.Graph()\n",
    "    with new_g.as_default():\n",
    "        a_new,b_new = tf.constant(20),tf.constant(30)\n",
    "        c_new = a_new+b_new\n",
    "        print(\"c_new：\\n\",c_new)\n",
    "        print('c_new的图属性：\\n',c_new.graph)\n",
    "        \n",
    "    # 开启new_g的会话\n",
    "    with tf.compat.v1.Session(graph=new_g) as new_sess:\n",
    "        c_new_value = new_sess.run(c_new)\n",
    "        print('c_new_value：\\n',c_new_value)\n",
    "        print('new_sess的图属性：\\n',new_sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_t:\n",
      " Tensor(\"Const_2:0\", shape=(), dtype=int32)\n",
      "b_t:\n",
      " Tensor(\"Const_3:0\", shape=(), dtype=int32)\n",
      "c_t：\n",
      " Tensor(\"add_2:0\", shape=(), dtype=int32)\n",
      "a_t的图属性：\n",
      " <tensorflow.python.framework.ops.Graph object at 0x000001A448A5E448>\n",
      "a_t的图属性：\n",
      " <tensorflow.python.framework.ops.Graph object at 0x000001A448A5E448>\n",
      "c_t_value：\n",
      " 5\n",
      "sess的图属性：\n",
      " <tensorflow.python.framework.ops.Graph object at 0x000001A448A5E448>\n",
      "c_new：\n",
      " Tensor(\"add:0\", shape=(), dtype=int32)\n",
      "c_new的图属性：\n",
      " <tensorflow.python.framework.ops.Graph object at 0x000001A448A8CE48>\n",
      "c_new_value：\n",
      " 50\n",
      "new_sess的图属性：\n",
      " <tensorflow.python.framework.ops.Graph object at 0x000001A448A8CE48>\n"
     ]
    }
   ],
   "source": [
    "# 两种图的地址不同\n",
    "# 每张图都有自己的命名空间\n",
    "graph_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.3 tensorboard\n",
    "- 1.数据序列化\n",
    "    tensorboard通过读取tensorflow的事件文件来运行，需要将数据生成一个序列化的summary protobuf对象\n",
    "    tf.summary.FileWriter(path,grap=sess.graph)\n",
    "- 2.启动tensorboard\n",
    "    tensorboard --logdir=path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#命令行开启\n",
    "# tensorboard --logdir=\"D:\\AliyunEDU\\04 summary\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.4 Op\n",
    "  数据：tensor对象\n",
    "  操作：operation对象 -Op\n",
    "   \n",
    "1.常见Op如下表\n",
    "\n",
    "|类型| 实例|\n",
    "|--|--|\n",
    "|标量运算| add, sub, mul, div, exp, log, greater, less, equal|\n",
    "|向量运算| concat, slice, splot. constant, rank, shape, shuffle|\n",
    "|矩阵运算| matmul, matrixinverse, matrixdateminant|\n",
    "|带状态的运算| Variable, assgin, assginadd|\n",
    "|神经网络组件| softmax, sigmoid relu,convolution,max_pool|\n",
    "|存储，恢复| Save, Restroe|\n",
    "|队列及同步运算| Enqueue, Dequeue, MutexAcquire, MutexReiease|\n",
    "|控制流| Merge, Switch, Enter, Leave, NextIteration|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.操作函数与操作对象\n",
    "    \n",
    "   |操作函数|操作对象|\n",
    "   |--|--|\n",
    "   |tf.constant(tensor对象)|输入tensor对象 -Const -输出tensor对象|\n",
    "   |tensor.add(tensor对象1,tensor对象2)|输入tensor对象1,tensor对象2 -Add对象 -输出tensor对象3|\n",
    "        \n",
    "     一个操作对象（Operation）是tensorflow图中的一个节点，可以接收0个或者多个输入tensor，并且可以输出0个或者多个tensor，operation对象是通过Op构造函数（如tf.matual()）创建的。\n",
    "     \n",
    "     例如：c = tf.constant(3.0)创建了一个Operation对象，类型为matmul类型，它将张量a，b作为输入，c作为输出，并且输出数据，打印的时候也是打印数据。其中，tf.matual()是函数，在执行matmul函数的过程中会通过matmul类创建一个与之对应的对象。\n",
    "     \n",
    "     注意，tf.Tensor 对象以输出该张量的tf.Operation明确命名。张量名称的形式为“<OP_NAME>:<i>”，其中：\n",
    "     “<OP_NAME>”是生成该张量的指令的名称\n",
    "     “<i>”是一个整数，它表示该张量在指令分输出中的索引\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.指令名称\n",
    "\n",
    "tf.Graph对象为其包含的tf.operation对象定义的一个命名空间。tensorflow会自动为图中的每个指令选择一个唯一的名称，用户也可以指定描述性名称，使程序阅读起来更轻松。我们可用操作方法中的`name`参数改写指令名称。例如：a_t = tf.constant(2,name='a_t')。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.1 Session\n",
    "tf.Session:用于完整的程序当中。tensorflow使用tf.Session类来表示客户端（通常为Python程序）与C++运行时之间的连接。\n",
    "tf.interactiveSession:用于交互式上下文中的tensor。\n",
    "\n",
    "1. __init__(target='',graph=None,config=None)\n",
    "会话可能拥有的资源，如tf.Variable,tf.QueueBase和tf.readerBase。当这些资源不再需要时，释放这些资源非常重要。因此，需要调用tf.Session.close会话中的方法，或将会话用作上下文管理器。\n",
    "   - target：如果将此参数留空（默认设置），会话将使用本地计算机中的设备。可以指定grpc://网址，以便确定tensorflow服务器的地址，这使得会话可以访问该服务器控制的计算机上的所有设备。\n",
    "   - graph=None： 运行默认图\n",
    "   - config：此参数允许在制定一个tf.ConfigProto以便控制会话的行为。例如，ConfigProto协议用于打印设备使用信息。\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "\n",
      "c_t_value：\n",
      " 5\n"
     ]
    }
   ],
   "source": [
    "def session_demo():\n",
    "    '''\n",
    "    会话的演示\n",
    "    :return:\n",
    "    '''\n",
    "    a_t,b_t = tf.constant(2),tf.constant(3)\n",
    "    c_t = a_t + b_t\n",
    "    \n",
    "    # 运行会话并打印设备信息\n",
    "    with tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(allow_soft_placement=True,\n",
    "                                                   log_device_placement=True)) as sess:\n",
    "        print(\"c_t_value：\\n\",sess.run(c_t)) \n",
    "\n",
    "session_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 会话中的run()\n",
    "\n",
    "`run(fetches,feed_dict=None,options=None.run_metadata=None)`\n",
    "\n",
    "- 通过run()来运行operation\n",
    "- fetches：单一的operation，或者列表、元组（其他不属于tensorflow的类型不行）\n",
    "- feed_dict：参数允许调用者覆盖图中张量的值，运行时赋值\n",
    "  - 与tf.placeholder搭配使用，则会检查值得形状是否与占位符兼容\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接上例，同时查看多个值:\n",
    "\n",
    "a,b,c = sess.run([a_t,b_t,c_t]) # 列表\n",
    "\n",
    "print(a,b,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. feed()\n",
    " - placeholder提供占位符，run时候通过feed_dict指定参数\n",
    " 未知数据样本维度时，可以先占位；当明确样本维度时，再用feed_dict加载。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def session_feed():\n",
    "    a_ph = tf.compat.v1.placeholder(tf.float32)\n",
    "    b_ph = tf.compat.v1.placeholder(tf.float32)\n",
    "    c_ph = tf.add(a_ph,b_ph)\n",
    "    print('c_ph:',c_ph)\n",
    "    \n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        # 运行placeholder\n",
    "        c_ph_value = sess.run(c_ph, feed_dict={a_ph:2, b_ph:3})\n",
    "        print(\"c_t_value：\\n\",c_ph_value) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_ph: Tensor(\"Add_4:0\", dtype=float32)\n",
      "c_t_value：\n",
      " 5.0\n"
     ]
    }
   ],
   "source": [
    "session_feed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 张量 Tensor\n",
    "tensorflow的张量就是一个你维数组，类型为tf.tensor。tensor具有以下两个重要的属性：\n",
    "- type:数据类型\n",
    "- shape:形状（阶）\n",
    "\n",
    "##### 2.4.1 张量的阶\n",
    "\n",
    "|阶|数学实例|Python|例子|\n",
    "|--|--|--|--|\n",
    "|0|纯量|只有大小|`a=64`|\n",
    "|1|向量|大小和方向|`v=[1,2,3]`|\n",
    "|2|矩阵|数据表|`m=[[1,2,3],[4,5,6]]`|\n",
    "|3|3阶张量|数据立体|`t=[[[1],[2]],[[3],[4]]]`|\n",
    "|n|n阶张量|...|...|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor1:\n",
      " Tensor(\"Const_6:0\", shape=(), dtype=float32)\n",
      "tensor2:\n",
      " Tensor(\"Const_7:0\", shape=(3,), dtype=int32)\n",
      "tensor3:\n",
      " Tensor(\"Const_8:0\", shape=(3, 1), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "def tensor_demo():\n",
    "    '''\n",
    "    张量的演示\n",
    "    '''\n",
    "    tensor1 = tf.constant(4.0)\n",
    "    tensor2 = tf.constant([1,2,3])\n",
    "    tensor3 = tf.constant([[4],[5],[6]], dtype=tf.int32)\n",
    "    \n",
    "    print(\"tensor1:\\n\",tensor1)\n",
    "    print(\"tensor2:\\n\",tensor2)\n",
    "    print(\"tensor3:\\n\",tensor3)\n",
    "    \n",
    "    return None\n",
    "    \n",
    "tensor_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.4.2 创建张量的指令\n",
    "1. 固定值张量\n",
    "\n",
    "tf.zeros(shape, dtype=tf.float32, name=None)\n",
    "\n",
    "tf.ones(shape, dtype=tf.float32, name=None)\n",
    "\n",
    "tf.constant(value, dtype=None,shape=None,name='Const')\n",
    "\n",
    "2. 随机值张量\n",
    "\n",
    "tf.random_normal(shape, mean=0.0, stddev=1.0,dtype=tf.float32,name=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.4.3 张量的变换\n",
    "\n",
    "回顾：ndarray属性的修改\n",
    "1. 类型的修改 \n",
    "  - ndarray.astype(type)\n",
    "  \n",
    "       tf.cast(tensor,dtype):不会改变原始的tensor，返回新的改变类型后的tensor\n",
    "       \n",
    "  - ndarray.tostring()\n",
    "\n",
    "\n",
    "2. 形状的修改\n",
    "  - ndarray.reshape(shape)\n",
    "    - 自动计算形状\n",
    "    - 返回一个新的数组\n",
    "    \n",
    "  - ndarray.resize(shape)\n",
    "    - 在原数组上修改\n",
    "    1.如何改变静态形状？\n",
    "      - 什么情况下采可以改变/更新静态形状？\n",
    "        只有在形状没有完全固定下来的情况下\n",
    "      - tensor.set_shape(shape)\n",
    "        在原来的形状上修改\n",
    "      \n",
    "    2.如何改变动态形状？\n",
    "      - tf.reshape(tensor,type)\n",
    "        返回一个新的数组\n",
    "        可以改变形状（行、列、维度），但不能改变元素数量！\n",
    "    \n",
    "1. tensorflow 类型修改\n",
    " - tf.to_string_to_number(string_tensor, out_type=None,name=None)\n",
    " - tf.to_double(x, name='ToDouble')\n",
    " - tf.to_float(x,name='ToFloat')\n",
    " - tf.to_bfloat16(x, name='ToBFloat16')\n",
    " - tf.to_int32(x,name='ToInt32')\n",
    " - tf.to_int64(x,name='ToInt64')\n",
    " - tf.cast(x,dtype,name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor3 before: Tensor(\"Const_9:0\", shape=(3, 1), dtype=int32)\n",
      "tensor3 after: Tensor(\"Cast:0\", shape=(3, 1), dtype=float32)\n",
      "a_pTensor(\"Placeholder_2:0\", shape=(None, None), dtype=float32)\n",
      " b_pTensor(\"Placeholder_3:0\", shape=(None, 10), dtype=float32)\n",
      " c_pTensor(\"Placeholder_4:0\", shape=(3, 2), dtype=float32)\n",
      "\n",
      "a_p_update: Tensor(\"Placeholder_2:0\", shape=(2, 3), dtype=float32)\n",
      "a_p_reshape: Tensor(\"Reshape:0\", shape=(2, 3, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def tensor_demo():\n",
    "    '''\n",
    "    张量类型的修改\n",
    "    '''\n",
    "\n",
    "    tensor = tf.constant([[4],[5],[6]], dtype=tf.int32)\n",
    "    \n",
    "    tensor_cast = tf.cast(tensor, dtype=tf.float32)\n",
    "    print(\"tensor3 before:\",tensor)\n",
    "    print(\"tensor3 after:\",tensor_cast)\n",
    "    \n",
    "    # 更新/改变静态形状\n",
    "    # 没有完全固定下来的静态形状\n",
    "    # shape中为None的维度，可以在以后的更新中改变，其余有固定值的维度无法改变\n",
    "    a_p = tf.compat.v1.placeholder(dtype=tf.float32, shape=[None,None])\n",
    "    b_p = tf.compat.v1.placeholder(dtype=tf.float32, shape=[None,10])\n",
    "    c_p = tf.compat.v1.placeholder(dtype=tf.float32, shape=[3,2])\n",
    "    print('a_p{}\\n b_p{}\\n c_p{}\\n'.format(a_p,b_p,c_p))\n",
    "    \n",
    "    # 更新形状未确定的部分\n",
    "    a_p.set_shape([2,3])\n",
    "    print('a_p_update:',a_p)\n",
    "    \n",
    "    # 动态形状修改\n",
    "    a_p = tf.compat.v1.placeholder(dtype=tf.float32, shape=[None,None])\n",
    "    a_p_reshape = tf.reshape(a_p,shape=[2,3,1])\n",
    "    print('a_p_reshape:',a_p_reshape)\n",
    "    return None\n",
    "    \n",
    "tensor_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. tensorflow 形状改变\n",
    "tensorflow的张量具有两种形状变换，动态形状和静态形状\n",
    "  - tf.reshape\n",
    "  - tf.set_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.4.4 张量的数学运算\n",
    "- 算数运算符\n",
    "- 基本数学函数\n",
    "- 矩阵运算\n",
    "- reduce操作\n",
    "- 序列索引操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 变量OP\n",
    "tensorflow变量是表示程序处理的共享持久状态的最佳方法。变量通过 tf.Variable OP类进行操作。变量的特点：\n",
    "- 存储持久化\n",
    "- 可修改值\n",
    "- 可指定被训练\n",
    "\n",
    "##### 2.5.1 创建变量\n",
    "tf.Variable(initial_value=None,trainable=True,collections=None,name=None)\n",
    "- initial_value:初始化的值\n",
    "- trainable:是否可训练\n",
    "- collections:新变量将添加到列出的图的集合中collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "a_value:\n",
      " 50\n"
     ]
    }
   ],
   "source": [
    "def variable_demo():\n",
    "    '''\n",
    "    变量的演示\n",
    "    '''\n",
    "    # 定义变量\n",
    "    a = tf.Variable(initial_value=50)\n",
    "    b = tf.Variable(initial_value=40)\n",
    "    c = tf.add(a,b)\n",
    "    \n",
    "    # 初始化变量\n",
    "    init = tf.compat.v1.global_variables_initializer()\n",
    "    \n",
    "    # 开启会话\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        # 初始化\n",
    "        sess.run(init)\n",
    "        a_value,b_value,c_value = sess.run([a,b,c])\n",
    "        print(\"a_value:\\n\", a_value)\n",
    "        \n",
    "    return None\n",
    "\n",
    "variable_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.5.2 使用tf.variable_scope()修改命名空间\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_value:\n",
      " 50\n"
     ]
    }
   ],
   "source": [
    "def variable_scope_demo():\n",
    "    '''\n",
    "    变量的演示\n",
    "    '''\n",
    "    # 使用命名空间定义变量\n",
    "    with tf.compat.v1.variable_scope(\"my_scope\"):\n",
    "        a = tf.Variable(initial_value=50)\n",
    "        b = tf.Variable(initial_value=40)\n",
    "        c = tf.add(a,b)\n",
    "    \n",
    "    # 初始化变量\n",
    "    init = tf.compat.v1.global_variables_initializer()\n",
    "    \n",
    "    # 开启会话\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        # 初始化\n",
    "        sess.run(init)\n",
    "        a_value,b_value,c_value = sess.run([a,b,c])\n",
    "        print(\"a_value:\\n\", a_value)\n",
    "        \n",
    "    return None\n",
    "\n",
    "variable_scope_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6 tensorflow API\n",
    "\n",
    "##### 2.6.1 基础API\n",
    "1. tf.app\n",
    "这个模块相当于为tensorflow进行的脚本提供一个main函数入口，可以定义脚本运行的flags。\n",
    "2. tf.image\n",
    "图像处理操作。主要是一些颜色变换、变形和图像的编码和解码。\n",
    "3. tf.gfile\n",
    "提供了一组文件操作函数。\n",
    "4. summary\n",
    "用来生成tensorboard可用的统计日志，主要提供了四种类型：audio、image、histogram、scalar\n",
    "5. tf.python_io\n",
    "用来读写TFRecords文件。\n",
    "6. tf.train\n",
    "提供了一些训练器，与tf.nn组合起来，实现一些网络的优化计算。\n",
    "7. tf.nn\n",
    "提供了一些构建神经网络的底层函数。比如卷积、池化等操作。\n",
    "\n",
    "##### 2.6.2 高级API\n",
    "1. tf.keras\n",
    "2. tf.layers\n",
    "提供更高级的概念层来定义一个模型。类似tf.keras\n",
    "3. tf.contrib（tf2.x已经弃用）\n",
    "tf.contrib.layers提供能够将计算图中网络层、正则化、摘要操作、是构建计算图的高级操作，但包含不稳定和实验代码，有可能会改变。Tensorflow2.x已经弃用！\n",
    "4. tf.estimator\n",
    "一个Estimator相当于model+training——evaluate的合体。在模块中，已经实现了几种简单的分类器和回归器，包括：Baseline,learning和dnn。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7 实例：实现线性回归训练\n",
    "##### 2.7.1线性回归\n",
    "- 1) 构建模型:\n",
    "   $y = w_1 x_1 + w_2 x_2 + ...+ w_n x_n + b$\n",
    "- 2) 构造损失函数:\n",
    "   均方误差\n",
    "- 3) 优化损失:\n",
    "   梯度下降\n",
    "\n",
    "##### 2.7.2 案例\n",
    "1. 准备真实数据\n",
    "  - x 特征值\n",
    "  - y_true 目标值\n",
    "  - y_true = 0.8x+0.7\n",
    "     \n",
    "\n",
    "2. 假定x 和y之间的关系满足\n",
    "  - y = kx + b\n",
    "  - k = 0.8 b = 0.7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "流程分析：\n",
    "(100, 1) * (1,1) = (100,1)\n",
    "y_predict = x * weights(1,1) + bias(1,1)\n",
    "\n",
    "- 1) 构建模型\n",
    "  y_predict = tf.matmul(x, weights) + bias\n",
    "  \n",
    "- 2) 构建损失函数\n",
    "  error = tf.reduce_mean(tf.square(y_predict - y_true))\n",
    "  \n",
    "- 3) 优化损失\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(error)训练过程其实是在更新迭代optimizer。\n",
    "  \n",
    "- 4) 学习率的设置、步数的设置与梯度爆炸\n",
    "  学习率越大，训练到较好结果的步数越小；学习率越小，训练到较好结果的步数越大。学习率过大会出现梯度爆炸的现象。\n",
    "\n",
    "如何解决梯度爆炸？\n",
    "  1. 重新设计网络\n",
    "  2. 调整学习率\n",
    "  3. 使用梯度截断（在训练过程中检查和限制梯度的大小）\n",
    "  4. 使用激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练前的模型参数为：权重：1.285268，偏置：-0.262539，损失为：1.327180\n",
      "第 1 次训练后的模型参数为：权重：1.236495，偏置：-0.160519，损失为：0.954453\n",
      "第 2 次训练后的模型参数为：权重：1.215682，偏置：-0.082345，损失为：0.814144\n",
      "第 3 次训练后的模型参数为：权重：1.188877，偏置：-0.007550，损失为：0.716614\n",
      "第 4 次训练后的模型参数为：权重：1.140042，偏置：0.069591，损失为：0.485564\n",
      "第 5 次训练后的模型参数为：权重：1.107437，偏置：0.131882，损失为：0.402303\n",
      "第 6 次训练后的模型参数为：权重：1.075560，偏置：0.186768，损失为：0.345032\n",
      "第 7 次训练后的模型参数为：权重：1.034928，偏置：0.242518，损失为：0.224279\n",
      "第 8 次训练后的模型参数为：权重：1.008416，偏置：0.290693，损失为：0.211818\n",
      "第 9 次训练后的模型参数为：权重：0.981889，偏置：0.332807，损失为：0.155043\n",
      "第 10 次训练后的模型参数为：权重：0.963760，偏置：0.371170，损失为：0.143988\n",
      "第 11 次训练后的模型参数为：权重：0.945340，偏置：0.404204，损失为：0.114266\n",
      "第 12 次训练后的模型参数为：权重：0.929358，偏置：0.431870，损失为：0.086132\n",
      "第 13 次训练后的模型参数为：权重：0.909629，偏置：0.461973，损失为：0.066212\n",
      "第 14 次训练后的模型参数为：权重：0.902202，偏置：0.484668，损失为：0.057334\n",
      "第 15 次训练后的模型参数为：权重：0.891466，偏置：0.506479，损失为：0.045086\n",
      "第 16 次训练后的模型参数为：权重：0.882639，偏置：0.525297，损失为：0.047699\n",
      "第 17 次训练后的模型参数为：权重：0.873741，偏置：0.543226，损失为：0.031146\n",
      "第 18 次训练后的模型参数为：权重：0.865607，偏置：0.559353，损失为：0.023736\n",
      "第 19 次训练后的模型参数为：权重：0.859370，偏置：0.573039，损失为：0.019687\n",
      "第 20 次训练后的模型参数为：权重：0.854197，偏置：0.585310，损失为：0.015149\n",
      "第 21 次训练后的模型参数为：权重：0.850756，偏置：0.596389，损失为：0.012536\n",
      "第 22 次训练后的模型参数为：权重：0.844455，偏置：0.606985，损失为：0.009137\n",
      "第 23 次训练后的模型参数为：权重：0.839410，偏置：0.616560，损失为：0.008347\n",
      "第 24 次训练后的模型参数为：权重：0.835339，偏置：0.625241，损失为：0.006479\n",
      "第 25 次训练后的模型参数为：权重：0.831893，偏置：0.632608，损失为：0.005329\n",
      "第 26 次训练后的模型参数为：权重：0.828922，偏置：0.639394，损失为：0.004471\n",
      "第 27 次训练后的模型参数为：权重：0.826139，偏置：0.645360，损失为：0.003552\n",
      "第 28 次训练后的模型参数为：权重：0.822718，偏置：0.650989，损失为：0.003156\n",
      "第 29 次训练后的模型参数为：权重：0.820899，偏置：0.655778，损失为：0.002300\n",
      "第 30 次训练后的模型参数为：权重：0.817500，偏置：0.660519，损失为：0.001960\n",
      "第 31 次训练后的模型参数为：权重：0.815949，偏置：0.664374，损失为：0.001462\n",
      "第 32 次训练后的模型参数为：权重：0.814362，偏置：0.667989，损失为：0.001187\n",
      "第 33 次训练后的模型参数为：权重：0.812391，偏置：0.671338，损失为：0.000992\n",
      "第 34 次训练后的模型参数为：权重：0.811748，偏置：0.673947，损失为：0.000790\n",
      "第 35 次训练后的模型参数为：权重：0.809830，偏置：0.676871，损失为：0.000724\n",
      "第 36 次训练后的模型参数为：权重：0.808685，偏置：0.679261，损失为：0.000576\n",
      "第 37 次训练后的模型参数为：权重：0.807795，偏置：0.681332，损失为：0.000392\n",
      "第 38 次训练后的模型参数为：权重：0.806987，偏置：0.683253，损失为：0.000311\n",
      "第 39 次训练后的模型参数为：权重：0.806705，偏置：0.684824，损失为：0.000313\n",
      "第 40 次训练后的模型参数为：权重：0.806165，偏置：0.686325，损失为：0.000227\n",
      "第 41 次训练后的模型参数为：权重：0.805558，偏置：0.687676，损失为：0.000164\n",
      "第 42 次训练后的模型参数为：权重：0.805053，偏置：0.688855，损失为：0.000153\n",
      "第 43 次训练后的模型参数为：权重：0.804437，偏置：0.689999，损失为：0.000108\n",
      "第 44 次训练后的模型参数为：权重：0.804091，偏置：0.690968，损失为：0.000100\n",
      "第 45 次训练后的模型参数为：权重：0.803609，偏置：0.691921，损失为：0.000078\n",
      "第 46 次训练后的模型参数为：权重：0.803328，偏置：0.692733，损失为：0.000059\n",
      "第 47 次训练后的模型参数为：权重：0.803042，偏置：0.693383，损失为：0.000065\n",
      "第 48 次训练后的模型参数为：权重：0.802817，偏置：0.694042，损失为：0.000043\n",
      "第 49 次训练后的模型参数为：权重：0.802537，偏置：0.694626，损失为：0.000032\n",
      "第 50 次训练后的模型参数为：权重：0.802331，偏置：0.695161，损失为：0.000025\n",
      "第 51 次训练后的模型参数为：权重：0.802050，偏置：0.695662，损失为：0.000022\n",
      "第 52 次训练后的模型参数为：权重：0.801860，偏置：0.696090，损失为：0.000018\n",
      "第 53 次训练后的模型参数为：权重：0.801696，偏置：0.696476，损失为：0.000014\n",
      "第 54 次训练后的模型参数为：权重：0.801544，偏置：0.696824，损失为：0.000013\n",
      "第 55 次训练后的模型参数为：权重：0.801330，偏置：0.697169，损失为：0.000010\n",
      "第 56 次训练后的模型参数为：权重：0.801212，偏置：0.697456，损失为：0.000008\n",
      "第 57 次训练后的模型参数为：权重：0.801075，偏置：0.697711，损失为：0.000006\n",
      "第 58 次训练后的模型参数为：权重：0.800968，偏置：0.697936，损失为：0.000005\n",
      "第 59 次训练后的模型参数为：权重：0.800841，偏置：0.698152，损失为：0.000004\n",
      "第 60 次训练后的模型参数为：权重：0.800772，偏置：0.698334，损失为：0.000003\n",
      "第 61 次训练后的模型参数为：权重：0.800696，偏置：0.698505，损失为：0.000003\n",
      "第 62 次训练后的模型参数为：权重：0.800616，偏置：0.698659，损失为：0.000002\n",
      "第 63 次训练后的模型参数为：权重：0.800564，偏置：0.698788，损失为：0.000002\n",
      "第 64 次训练后的模型参数为：权重：0.800505，偏置：0.698914，损失为：0.000002\n",
      "第 65 次训练后的模型参数为：权重：0.800466，偏置：0.699023，损失为：0.000001\n",
      "第 66 次训练后的模型参数为：权重：0.800409，偏置：0.699127，损失为：0.000001\n",
      "第 67 次训练后的模型参数为：权重：0.800358，偏置：0.699219，损失为：0.000001\n",
      "第 68 次训练后的模型参数为：权重：0.800321，偏置：0.699300，损失为：0.000001\n",
      "第 69 次训练后的模型参数为：权重：0.800300，偏置：0.699367，损失为：0.000000\n",
      "第 70 次训练后的模型参数为：权重：0.800269，偏置：0.699432，损失为：0.000000\n",
      "第 71 次训练后的模型参数为：权重：0.800250，偏置：0.699487，损失为：0.000000\n",
      "第 72 次训练后的模型参数为：权重：0.800225，偏置：0.699540，损失为：0.000000\n",
      "第 73 次训练后的模型参数为：权重：0.800202，偏置：0.699587，损失为：0.000000\n",
      "第 74 次训练后的模型参数为：权重：0.800182，偏置：0.699628，损失为：0.000000\n",
      "第 75 次训练后的模型参数为：权重：0.800165，偏置：0.699667，损失为：0.000000\n",
      "第 76 次训练后的模型参数为：权重：0.800142，偏置：0.699703，损失为：0.000000\n",
      "第 77 次训练后的模型参数为：权重：0.800129，偏置：0.699732，损失为：0.000000\n",
      "第 78 次训练后的模型参数为：权重：0.800113，偏置：0.699760，损失为：0.000000\n",
      "第 79 次训练后的模型参数为：权重：0.800104，偏置：0.699784，损失为：0.000000\n",
      "第 80 次训练后的模型参数为：权重：0.800091，偏置：0.699807，损失为：0.000000\n",
      "第 81 次训练后的模型参数为：权重：0.800083，偏置：0.699825，损失为：0.000000\n",
      "第 82 次训练后的模型参数为：权重：0.800075，偏置：0.699843，损失为：0.000000\n",
      "第 83 次训练后的模型参数为：权重：0.800070，偏置：0.699858，损失为：0.000000\n",
      "第 84 次训练后的模型参数为：权重：0.800061，偏置：0.699872，损失为：0.000000\n",
      "第 85 次训练后的模型参数为：权重：0.800054，偏置：0.699885，损失为：0.000000\n",
      "第 86 次训练后的模型参数为：权重：0.800050，偏置：0.699896，损失为：0.000000\n",
      "第 87 次训练后的模型参数为：权重：0.800047，偏置：0.699906，损失为：0.000000\n",
      "第 88 次训练后的模型参数为：权重：0.800042，偏置：0.699916，损失为：0.000000\n",
      "第 89 次训练后的模型参数为：权重：0.800038，偏置：0.699924，损失为：0.000000\n",
      "第 90 次训练后的模型参数为：权重：0.800034，偏置：0.699932，损失为：0.000000\n",
      "第 91 次训练后的模型参数为：权重：0.800032，偏置：0.699938，损失为：0.000000\n",
      "第 92 次训练后的模型参数为：权重：0.800029，偏置：0.699945，损失为：0.000000\n",
      "第 93 次训练后的模型参数为：权重：0.800025，偏置：0.699950，损失为：0.000000\n",
      "第 94 次训练后的模型参数为：权重：0.800024，偏置：0.699954，损失为：0.000000\n",
      "第 95 次训练后的模型参数为：权重：0.800021，偏置：0.699959，损失为：0.000000\n",
      "第 96 次训练后的模型参数为：权重：0.800019，偏置：0.699963，损失为：0.000000\n",
      "第 97 次训练后的模型参数为：权重：0.800017，偏置：0.699967，损失为：0.000000\n",
      "第 98 次训练后的模型参数为：权重：0.800015，偏置：0.699970，损失为：0.000000\n",
      "第 99 次训练后的模型参数为：权重：0.800013，偏置：0.699973，损失为：0.000000\n",
      "第 100 次训练后的模型参数为：权重：0.800012，偏置：0.699976，损失为：0.000000\n",
      "第 101 次训练后的模型参数为：权重：0.800011，偏置：0.699978，损失为：0.000000\n",
      "第 102 次训练后的模型参数为：权重：0.800010，偏置：0.699981，损失为：0.000000\n",
      "第 103 次训练后的模型参数为：权重：0.800008，偏置：0.699983，损失为：0.000000\n",
      "第 104 次训练后的模型参数为：权重：0.800008，偏置：0.699984，损失为：0.000000\n",
      "第 105 次训练后的模型参数为：权重：0.800007，偏置：0.699986，损失为：0.000000\n",
      "第 106 次训练后的模型参数为：权重：0.800006，偏置：0.699987，损失为：0.000000\n",
      "第 107 次训练后的模型参数为：权重：0.800006，偏置：0.699989，损失为：0.000000\n",
      "第 108 次训练后的模型参数为：权重：0.800005，偏置：0.699990，损失为：0.000000\n",
      "第 109 次训练后的模型参数为：权重：0.800005，偏置：0.699991，损失为：0.000000\n",
      "第 110 次训练后的模型参数为：权重：0.800004，偏置：0.699992，损失为：0.000000\n",
      "第 111 次训练后的模型参数为：权重：0.800004，偏置：0.699993，损失为：0.000000\n",
      "第 112 次训练后的模型参数为：权重：0.800003，偏置：0.699993，损失为：0.000000\n",
      "第 113 次训练后的模型参数为：权重：0.800003，偏置：0.699994，损失为：0.000000\n",
      "第 114 次训练后的模型参数为：权重：0.800003，偏置：0.699995，损失为：0.000000\n",
      "第 115 次训练后的模型参数为：权重：0.800002，偏置：0.699995，损失为：0.000000\n",
      "第 116 次训练后的模型参数为：权重：0.800002，偏置：0.699996，损失为：0.000000\n",
      "第 117 次训练后的模型参数为：权重：0.800002，偏置：0.699996，损失为：0.000000\n",
      "第 118 次训练后的模型参数为：权重：0.800002，偏置：0.699996，损失为：0.000000\n",
      "第 119 次训练后的模型参数为：权重：0.800002，偏置：0.699997，损失为：0.000000\n",
      "第 120 次训练后的模型参数为：权重：0.800001，偏置：0.699997，损失为：0.000000\n",
      "第 121 次训练后的模型参数为：权重：0.800001，偏置：0.699997，损失为：0.000000\n",
      "第 122 次训练后的模型参数为：权重：0.800001，偏置：0.699998，损失为：0.000000\n",
      "第 123 次训练后的模型参数为：权重：0.800001，偏置：0.699998，损失为：0.000000\n",
      "第 124 次训练后的模型参数为：权重：0.800001，偏置：0.699998，损失为：0.000000\n",
      "第 125 次训练后的模型参数为：权重：0.800001，偏置：0.699998，损失为：0.000000\n",
      "第 126 次训练后的模型参数为：权重：0.800001，偏置：0.699998，损失为：0.000000\n",
      "第 127 次训练后的模型参数为：权重：0.800001，偏置：0.699999，损失为：0.000000\n",
      "第 128 次训练后的模型参数为：权重：0.800001，偏置：0.699999，损失为：0.000000\n",
      "第 129 次训练后的模型参数为：权重：0.800001，偏置：0.699999，损失为：0.000000\n",
      "第 130 次训练后的模型参数为：权重：0.800001，偏置：0.699999，损失为：0.000000\n",
      "第 131 次训练后的模型参数为：权重：0.800000，偏置：0.699999，损失为：0.000000\n",
      "第 132 次训练后的模型参数为：权重：0.800000，偏置：0.699999，损失为：0.000000\n",
      "第 133 次训练后的模型参数为：权重：0.800000，偏置：0.699999，损失为：0.000000\n",
      "第 134 次训练后的模型参数为：权重：0.800000，偏置：0.699999，损失为：0.000000\n",
      "第 135 次训练后的模型参数为：权重：0.800000，偏置：0.699999，损失为：0.000000\n",
      "第 136 次训练后的模型参数为：权重：0.800000，偏置：0.699999，损失为：0.000000\n",
      "第 137 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 138 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 139 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 140 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 141 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 142 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 143 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 144 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 145 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 146 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 147 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 148 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 149 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 150 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 151 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 152 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 153 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 154 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 155 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 156 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 157 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 158 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 159 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 160 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 161 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 162 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 163 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 164 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 165 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 166 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 167 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 168 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 169 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 170 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 171 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 172 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 173 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 174 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 175 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 176 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 177 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 178 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 179 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 180 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 181 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 182 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 183 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 184 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 185 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 186 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 187 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 188 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 189 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 190 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 191 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 192 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 193 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 194 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 195 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 196 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 197 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 198 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 199 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 200 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n"
     ]
    }
   ],
   "source": [
    "def linear_regression():\n",
    "    # 1.准备数据\n",
    "    X = tf.compat.v1.random_normal(shape=[100,1])\n",
    "    y_true = tf.matmul(X, [[0.8]]) + 0.7\n",
    "    \n",
    "    # 2.构造模型\n",
    "    # 定义模型参数用 变量Variable\n",
    "    weights = tf.Variable(initial_value=tf.compat.v1.random_normal(shape=[1,1]))\n",
    "    bias = tf.Variable(initial_value=tf.compat.v1.random_normal(shape=[1,1]))\n",
    "    y_predict = tf.matmul(X,weights) + bias\n",
    "    \n",
    "    # 3.构造损失函数\n",
    "    error = tf.reduce_mean(tf.square(y_predict - y_true))\n",
    "    \n",
    "    # 4.优化损失\n",
    "    optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.05).minimize(error)\n",
    "    \n",
    "    # 2_收集变量\n",
    "    tf.compat.v1.summary.scalar(\"error\",error)\n",
    "    tf.compat.v1.summary.histogram(\"weights\",weights)\n",
    "    tf.compat.v1.summary.histogram(\"bias\",bias)\n",
    "    \n",
    "    # 3_合并变量\n",
    "    merged = tf.compat.v1.summary.merge_all()\n",
    "    \n",
    "    # 显式的初始化变量\n",
    "    init  = tf.compat.v1.global_variables_initializer()\n",
    "    \n",
    "    # 开启会话\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        # 初始化变量\n",
    "        sess.run(init)\n",
    "        \n",
    "        # 1_创建事件文件\n",
    "        file_writer = tf.compat.v1.summary.FileWriter(\"D:/AliyunEDU/04 summary2/\",graph=sess.graph)\n",
    "        \n",
    "        # 查看初始化模型参数后的值\n",
    "        print(\"训练前的模型参数为：权重：%f，偏置：%f，损失为：%f\" % (weights.eval(), bias.eval(), error.eval()))\n",
    "        \n",
    "        # 开始训练\n",
    "        for i in range(200):\n",
    "            sess.run(optimizer)\n",
    "            print(\"第 %d 次训练后的模型参数为：权重：%f，偏置：%f，损失为：%f\" % (i+1,weights.eval(), bias.eval(), error.eval()))\n",
    "        \n",
    "            # 4_运行合并变量操作\n",
    "            summary = sess.run(merged)\n",
    "            \n",
    "            # 5_将每次迭代后的变量写入事件文件\n",
    "            file_writer.add_summary(summary,i)\n",
    "    return None\n",
    "\n",
    "linear_regression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.7.3增加其他功能\n",
    "在tensorboard中观察模型的参数、损失值等变量值的变化\n",
    "\n",
    "1. 收集变量\n",
    "- tf.summary.scalar(name='',tensor)收集对于损失函数和准确率等单值变量，name为变量的名字，tensor为值。\n",
    "\n",
    "- tf.summary.histogram(name=''tensor)收集高维度的变量参数\n",
    "\n",
    "- tf.summary.image(name='',tensor)\n",
    "\n",
    "2.合并变量并写入事件文件\n",
    "- merged = tf.summary.merge_all()\n",
    "- 运行合并：summary = sess.run(merged)每次迭代都运行\n",
    "- 添加：FileWriter.add_summary(summary,i) i表示第几次的值\n",
    "\n",
    "总结：\n",
    "- 1.创建事件文件\n",
    "- 2.收集变量\n",
    "- 3.合并变量\n",
    "- 4.每次迭代运行一次合并变量\n",
    "- 5.每次迭代将summary对象写入事件文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.7.4 添加命名空间\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练前的模型参数为：权重：-0.258935，偏置：0.667049，损失为：1.199042\n",
      "第 1 次训练后的模型参数为：权重：-0.142458，偏置：0.661531，损失为：0.832151\n",
      "第 2 次训练后的模型参数为：权重：-0.039418，偏置：0.640315，损失为：0.608705\n",
      "第 3 次训练后的模型参数为：权重：0.030339，偏置：0.651682，损失为：0.644915\n",
      "第 4 次训练后的模型参数为：权重：0.111354，偏置：0.669000，损失为：0.411763\n",
      "第 5 次训练后的模型参数为：权重：0.180950，偏置：0.669974，损失为：0.339904\n",
      "第 6 次训练后的模型参数为：权重：0.242945，偏置：0.681054，损失为：0.365621\n",
      "第 7 次训练后的模型参数为：权重：0.293985，偏置：0.691853，损失为：0.239445\n",
      "第 8 次训练后的模型参数为：权重：0.345628，偏置：0.692595，损失为：0.178773\n",
      "第 9 次训练后的模型参数为：权重：0.382778，偏置：0.699669，损失为：0.182599\n",
      "第 10 次训练后的模型参数为：权重：0.420882，偏置：0.702772，损失为：0.108335\n",
      "第 11 次训练后的模型参数为：权重：0.452248，偏置：0.703719，损失为：0.102443\n",
      "第 12 次训练后的模型参数为：权重：0.489996，偏置：0.695170，损失为：0.109332\n",
      "第 13 次训练后的模型参数为：权重：0.523862，偏置：0.696821，损失为：0.086174\n",
      "第 14 次训练后的模型参数为：权重：0.547956，偏置：0.693637，损失为：0.075707\n",
      "第 15 次训练后的模型参数为：权重：0.571418，偏置：0.697458，损失为：0.048406\n",
      "第 16 次训练后的模型参数为：权重：0.593478，偏置：0.700902，损失为：0.045874\n",
      "第 17 次训练后的模型参数为：权重：0.614029，偏置：0.699527，损失为：0.033849\n",
      "第 18 次训练后的模型参数为：权重：0.631859，偏置：0.698342，损失为：0.032264\n",
      "第 19 次训练后的模型参数为：权重：0.646254，偏置：0.698117，损失为：0.024444\n",
      "第 20 次训练后的模型参数为：权重：0.663854，偏置：0.695927，损失为：0.016646\n",
      "第 21 次训练后的模型参数为：权重：0.675621，偏置：0.697702，损失为：0.018144\n",
      "第 22 次训练后的模型参数为：权重：0.685579，偏置：0.699745，损失为：0.013945\n",
      "第 23 次训练后的模型参数为：权重：0.695577，偏置：0.701507，损失为：0.012465\n",
      "第 24 次训练后的模型参数为：权重：0.706230，偏置：0.702013，损失为：0.009473\n",
      "第 25 次训练后的模型参数为：权重：0.716669，偏置：0.701377，损失为：0.007668\n",
      "第 26 次训练后的模型参数为：权重：0.725388，偏置：0.702631，损失为：0.005186\n",
      "第 27 次训练后的模型参数为：权重：0.732317，偏置：0.700371，损失为：0.004573\n",
      "第 28 次训练后的模型参数为：权重：0.739197，偏置：0.700179，损失为：0.003886\n",
      "第 29 次训练后的模型参数为：权重：0.746955，偏置：0.700358，损失为：0.002561\n",
      "第 30 次训练后的模型参数为：权重：0.751610，偏置：0.700224，损失为：0.002456\n",
      "第 31 次训练后的模型参数为：权重：0.756327，偏置：0.699810，损失为：0.001870\n",
      "第 32 次训练后的模型参数为：权重：0.761220，偏置：0.699408，损失为：0.001742\n",
      "第 33 次训练后的模型参数为：权重：0.764030，偏置：0.699361，损失为：0.001030\n",
      "第 34 次训练后的模型参数为：权重：0.768565，偏置：0.699352，损失为：0.000943\n",
      "第 35 次训练后的模型参数为：权重：0.771968，偏置：0.699351，损失为：0.000726\n",
      "第 36 次训练后的模型参数为：权重：0.774671，偏置：0.699862，损失为：0.000653\n",
      "第 37 次训练后的模型参数为：权重：0.776944，偏置：0.699734，损失为：0.000518\n",
      "第 38 次训练后的模型参数为：权重：0.778982，偏置：0.699675，损失为：0.000461\n",
      "第 39 次训练后的模型参数为：权重：0.780770，偏置：0.699483，损失为：0.000357\n",
      "第 40 次训练后的模型参数为：权重：0.782960，偏置：0.699304，损失为：0.000330\n",
      "第 41 次训练后的模型参数为：权重：0.784665，偏置：0.699303，损失为：0.000272\n",
      "第 42 次训练后的模型参数为：权重：0.786168，偏置：0.699420，损失为：0.000174\n",
      "第 43 次训练后的模型参数为：权重：0.787572，偏置：0.699481，损失为：0.000119\n",
      "第 44 次训练后的模型参数为：权重：0.788687，偏置：0.699607，损失为：0.000125\n",
      "第 45 次训练后的模型参数为：权重：0.789864，偏置：0.699574，损失为：0.000108\n",
      "第 46 次训练后的模型参数为：权重：0.790855，偏置：0.699568，损失为：0.000094\n",
      "第 47 次训练后的模型参数为：权重：0.791810，偏置：0.699547，损失为：0.000075\n",
      "第 48 次训练后的模型参数为：权重：0.792592，偏置：0.699578，损失为：0.000060\n",
      "第 49 次训练后的模型参数为：权重：0.793434，偏置：0.699567，损失为：0.000033\n",
      "第 50 次训练后的模型参数为：权重：0.794077，偏置：0.699659，损失为：0.000027\n",
      "第 51 次训练后的模型参数为：权重：0.794619，偏置：0.699656，损失为：0.000031\n",
      "第 52 次训练后的模型参数为：权重：0.795243，偏置：0.699752，损失为：0.000023\n",
      "第 53 次训练后的模型参数为：权重：0.795663，偏置：0.699819，损失为：0.000019\n",
      "第 54 次训练后的模型参数为：权重：0.796035，偏置：0.699867，损失为：0.000014\n",
      "第 55 次训练后的模型参数为：权重：0.796403，偏置：0.699866，损失为：0.000012\n",
      "第 56 次训练后的模型参数为：权重：0.796744，偏置：0.699888，损失为：0.000010\n",
      "第 57 次训练后的模型参数为：权重：0.797004，偏置：0.699898，损失为：0.000009\n",
      "第 58 次训练后的模型参数为：权重：0.797236，偏置：0.699964，损失为：0.000008\n",
      "第 59 次训练后的模型参数为：权重：0.797477，偏置：0.699968，损失为：0.000006\n",
      "第 60 次训练后的模型参数为：权重：0.797764，偏置：0.699965，损失为：0.000005\n",
      "第 61 次训练后的模型参数为：权重：0.797980，偏置：0.699998，损失为：0.000003\n",
      "第 62 次训练后的模型参数为：权重：0.798159，偏置：0.700002，损失为：0.000004\n",
      "第 63 次训练后的模型参数为：权重：0.798330，偏置：0.700017，损失为：0.000003\n",
      "第 64 次训练后的模型参数为：权重：0.798488，偏置：0.699996，损失为：0.000002\n",
      "第 65 次训练后的模型参数为：权重：0.798635，偏置：0.699962，损失为：0.000002\n",
      "第 66 次训练后的模型参数为：权重：0.798772，偏置：0.699988，损失为：0.000001\n",
      "第 67 次训练后的模型参数为：权重：0.798901，偏置：0.699961，损失为：0.000001\n",
      "第 68 次训练后的模型参数为：权重：0.798989，偏置：0.699962，损失为：0.000001\n",
      "第 69 次训练后的模型参数为：权重：0.799080，偏置：0.699955，损失为：0.000001\n",
      "第 70 次训练后的模型参数为：权重：0.799163，偏置：0.699958，损失为：0.000001\n",
      "第 71 次训练后的模型参数为：权重：0.799243，偏置：0.699951，损失为：0.000001\n",
      "第 72 次训练后的模型参数为：权重：0.799332，偏置：0.699962，损失为：0.000000\n",
      "第 73 次训练后的模型参数为：权重：0.799393，偏置：0.699966，损失为：0.000000\n",
      "第 74 次训练后的模型参数为：权重：0.799470，偏置：0.699963，损失为：0.000000\n",
      "第 75 次训练后的模型参数为：权重：0.799523，偏置：0.699977，损失为：0.000000\n",
      "第 76 次训练后的模型参数为：权重：0.799568，偏置：0.699983，损失为：0.000000\n",
      "第 77 次训练后的模型参数为：权重：0.799612，偏置：0.699979，损失为：0.000000\n",
      "第 78 次训练后的模型参数为：权重：0.799649，偏置：0.699978，损失为：0.000000\n",
      "第 79 次训练后的模型参数为：权重：0.799689，偏置：0.699981，损失为：0.000000\n",
      "第 80 次训练后的模型参数为：权重：0.799728，偏置：0.699982，损失为：0.000000\n",
      "第 81 次训练后的模型参数为：权重：0.799755，偏置：0.699977，损失为：0.000000\n",
      "第 82 次训练后的模型参数为：权重：0.799776，偏置：0.699979，损失为：0.000000\n",
      "第 83 次训练后的模型参数为：权重：0.799798，偏置：0.699980，损失为：0.000000\n",
      "第 84 次训练后的模型参数为：权重：0.799817，偏置：0.699981，损失为：0.000000\n",
      "第 85 次训练后的模型参数为：权重：0.799833，偏置：0.699983，损失为：0.000000\n",
      "第 86 次训练后的模型参数为：权重：0.799850，偏置：0.699985，损失为：0.000000\n",
      "第 87 次训练后的模型参数为：权重：0.799865，偏置：0.699988，损失为：0.000000\n",
      "第 88 次训练后的模型参数为：权重：0.799881，偏置：0.699990，损失为：0.000000\n",
      "第 89 次训练后的模型参数为：权重：0.799891，偏置：0.699990，损失为：0.000000\n",
      "第 90 次训练后的模型参数为：权重：0.799903，偏置：0.699989，损失为：0.000000\n",
      "第 91 次训练后的模型参数为：权重：0.799912，偏置：0.699988，损失为：0.000000\n",
      "第 92 次训练后的模型参数为：权重：0.799922，偏置：0.699990，损失为：0.000000\n",
      "第 93 次训练后的模型参数为：权重：0.799930，偏置：0.699991，损失为：0.000000\n",
      "第 94 次训练后的模型参数为：权重：0.799937，偏置：0.699993，损失为：0.000000\n",
      "第 95 次训练后的模型参数为：权重：0.799943，偏置：0.699994，损失为：0.000000\n",
      "第 96 次训练后的模型参数为：权重：0.799948，偏置：0.699994，损失为：0.000000\n",
      "第 97 次训练后的模型参数为：权重：0.799952，偏置：0.699995，损失为：0.000000\n",
      "第 98 次训练后的模型参数为：权重：0.799957，偏置：0.699996，损失为：0.000000\n",
      "第 99 次训练后的模型参数为：权重：0.799961，偏置：0.699996，损失为：0.000000\n",
      "第 100 次训练后的模型参数为：权重：0.799965，偏置：0.699996，损失为：0.000000\n",
      "第 101 次训练后的模型参数为：权重：0.799968，偏置：0.699996，损失为：0.000000\n",
      "第 102 次训练后的模型参数为：权重：0.799972，偏置：0.699996，损失为：0.000000\n",
      "第 103 次训练后的模型参数为：权重：0.799974，偏置：0.699997，损失为：0.000000\n",
      "第 104 次训练后的模型参数为：权重：0.799977，偏置：0.699997，损失为：0.000000\n",
      "第 105 次训练后的模型参数为：权重：0.799979，偏置：0.699997，损失为：0.000000\n",
      "第 106 次训练后的模型参数为：权重：0.799981，偏置：0.699997，损失为：0.000000\n",
      "第 107 次训练后的模型参数为：权重：0.799983，偏置：0.699997，损失为：0.000000\n",
      "第 108 次训练后的模型参数为：权重：0.799985，偏置：0.699998，损失为：0.000000\n",
      "第 109 次训练后的模型参数为：权重：0.799987，偏置：0.699998，损失为：0.000000\n",
      "第 110 次训练后的模型参数为：权重：0.799988，偏置：0.699998，损失为：0.000000\n",
      "第 111 次训练后的模型参数为：权重：0.799989，偏置：0.699998，损失为：0.000000\n",
      "第 112 次训练后的模型参数为：权重：0.799990，偏置：0.699998，损失为：0.000000\n",
      "第 113 次训练后的模型参数为：权重：0.799991，偏置：0.699999，损失为：0.000000\n",
      "第 114 次训练后的模型参数为：权重：0.799992，偏置：0.699999，损失为：0.000000\n",
      "第 115 次训练后的模型参数为：权重：0.799993，偏置：0.699999，损失为：0.000000\n",
      "第 116 次训练后的模型参数为：权重：0.799994，偏置：0.699999，损失为：0.000000\n",
      "第 117 次训练后的模型参数为：权重：0.799994，偏置：0.699999，损失为：0.000000\n",
      "第 118 次训练后的模型参数为：权重：0.799995，偏置：0.699999，损失为：0.000000\n",
      "第 119 次训练后的模型参数为：权重：0.799995，偏置：0.699999，损失为：0.000000\n",
      "第 120 次训练后的模型参数为：权重：0.799996，偏置：0.699999，损失为：0.000000\n",
      "第 121 次训练后的模型参数为：权重：0.799996，偏置：0.699999，损失为：0.000000\n",
      "第 122 次训练后的模型参数为：权重：0.799997，偏置：0.699999，损失为：0.000000\n",
      "第 123 次训练后的模型参数为：权重：0.799997，偏置：0.699999，损失为：0.000000\n",
      "第 124 次训练后的模型参数为：权重：0.799997，偏置：0.699999，损失为：0.000000\n",
      "第 125 次训练后的模型参数为：权重：0.799998，偏置：0.700000，损失为：0.000000\n",
      "第 126 次训练后的模型参数为：权重：0.799998，偏置：0.700000，损失为：0.000000\n",
      "第 127 次训练后的模型参数为：权重：0.799998，偏置：0.700000，损失为：0.000000\n",
      "第 128 次训练后的模型参数为：权重：0.799998，偏置：0.700000，损失为：0.000000\n",
      "第 129 次训练后的模型参数为：权重：0.799998，偏置：0.700000，损失为：0.000000\n",
      "第 130 次训练后的模型参数为：权重：0.799999，偏置：0.700000，损失为：0.000000\n",
      "第 131 次训练后的模型参数为：权重：0.799999，偏置：0.700000，损失为：0.000000\n",
      "第 132 次训练后的模型参数为：权重：0.799999，偏置：0.700000，损失为：0.000000\n",
      "第 133 次训练后的模型参数为：权重：0.799999，偏置：0.700000，损失为：0.000000\n",
      "第 134 次训练后的模型参数为：权重：0.799999，偏置：0.700000，损失为：0.000000\n",
      "第 135 次训练后的模型参数为：权重：0.799999，偏置：0.700000，损失为：0.000000\n",
      "第 136 次训练后的模型参数为：权重：0.799999，偏置：0.700000，损失为：0.000000\n",
      "第 137 次训练后的模型参数为：权重：0.799999，偏置：0.700000，损失为：0.000000\n",
      "第 138 次训练后的模型参数为：权重：0.799999，偏置：0.700000，损失为：0.000000\n",
      "第 139 次训练后的模型参数为：权重：0.799999，偏置：0.700000，损失为：0.000000\n",
      "第 140 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 141 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 142 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 143 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 144 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 145 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 146 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 147 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 148 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 149 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 150 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 151 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 152 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 153 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 154 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 155 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 156 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 157 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 158 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 159 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 160 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 161 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 162 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 163 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 164 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 165 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 166 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 167 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 168 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 169 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 170 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 171 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 172 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 173 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 174 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 175 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 176 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 177 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 178 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 179 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 180 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 181 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 182 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 183 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 184 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 185 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 186 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 187 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 188 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 189 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 190 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 191 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 192 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 193 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 194 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 195 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 196 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 197 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 198 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 199 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n",
      "第 200 次训练后的模型参数为：权重：0.800000，偏置：0.700000，损失为：0.000000\n"
     ]
    }
   ],
   "source": [
    "def linear_regression_scope():\n",
    "    \n",
    "    # 1.准备数据\n",
    "    with tf.compat.v1.variable_scope(\"prepare_data\"):\n",
    "        X = tf.compat.v1.random_normal(shape=[100,1],name=\"X\")\n",
    "        y_true = tf.matmul(X, [[0.8]]) + 0.7\n",
    "    \n",
    "    # 2.构造模型\n",
    "    # 定义模型参数用 变量Variable\n",
    "    with tf.compat.v1.variable_scope(\"create_model\"):\n",
    "        weights = tf.Variable(initial_value=tf.compat.v1.random_normal(shape=[1,1]),name=\"Weights\")\n",
    "        bias = tf.Variable(initial_value=tf.compat.v1.random_normal(shape=[1,1]),name=\"Bias\")\n",
    "        y_predict = tf.matmul(X,weights) + bias\n",
    "    \n",
    "    # 3.构造损失函数\n",
    "    with tf.compat.v1.variable_scope(\"loss_function\"):\n",
    "        error = tf.reduce_mean(tf.square(y_predict - y_true))\n",
    "    \n",
    "    # 4.优化损失\n",
    "    with tf.compat.v1.variable_scope(\"optimizer\"):\n",
    "        optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.05).minimize(error)\n",
    "    \n",
    "    # 2_收集变量\n",
    "    tf.compat.v1.summary.scalar(\"error\",error)\n",
    "    tf.compat.v1.summary.histogram(\"weights\",weights)\n",
    "    tf.compat.v1.summary.histogram(\"bias\",bias)\n",
    "    \n",
    "    # 3_合并变量\n",
    "    merged = tf.compat.v1.summary.merge_all()\n",
    "    \n",
    "    # 显式的初始化变量\n",
    "    init  = tf.compat.v1.global_variables_initializer()\n",
    "    \n",
    "    # 开启会话\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        # 初始化变量\n",
    "        sess.run(init)\n",
    "        \n",
    "        # 1_创建事件文件\n",
    "        file_writer = tf.compat.v1.summary.FileWriter(\"D:/AliyunEDU/04 summary2/\",graph=sess.graph)\n",
    "        \n",
    "        # 查看初始化模型参数后的值\n",
    "        print(\"训练前的模型参数为：权重：%f，偏置：%f，损失为：%f\" % (weights.eval(), bias.eval(), error.eval()))\n",
    "        \n",
    "        # 开始训练\n",
    "        for i in range(200):\n",
    "            sess.run(optimizer)\n",
    "            print(\"第 %d 次训练后的模型参数为：权重：%f，偏置：%f，损失为：%f\" % (i+1,weights.eval(), bias.eval(), error.eval()))\n",
    "        \n",
    "            # 4_运行合并变量操作\n",
    "            summary = sess.run(merged)\n",
    "            \n",
    "            # 5_将每次迭代后的变量写入事件文件\n",
    "            file_writer.add_summary(summary,i)\n",
    "    return None\n",
    "\n",
    "linear_regression_scope()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7.5 模型的保存与加载"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.train.Saver(var_list=None,max_to_keep=5)\n",
    "- 保存和加载模型（保存文件格式：checkpoint）\n",
    "- var_list:指定将要保存和还原的变量。他可以作为一个dict或一个列表传递。\n",
    "- max_to_keep:指示要保留的最近检查点文件的最大数量。创建新文件时，会删除较旧的文件。如果无或0，则保留所有检查点文件。默认为5。\n",
    "    \n",
    "使用步骤：\n",
    "1. saver = tf.train.Saver(var_list=None,max_to_keep=5)\n",
    "2. 保存：saver.save(sess, path)#需要先创建文件夹\n",
    "3. 加载：saver.restore(sess,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练前的模型参数为：权重：0.187838，偏置：0.189586，损失为：0.454763\n",
      "训练后的模型训练参数为:权重0.187838，偏置0.189586，损失0.630524\n"
     ]
    }
   ],
   "source": [
    "def linear_regression_scope_save():\n",
    "    \n",
    "    # 1.准备数据\n",
    "    with tf.compat.v1.variable_scope(\"prepare_data\"):\n",
    "        X = tf.compat.v1.random_normal(shape=[100,1],name=\"X\")\n",
    "        y_true = tf.matmul(X, [[0.8]]) + 0.7\n",
    "    \n",
    "    # 2.构造模型\n",
    "    # 定义模型参数用 变量Variable\n",
    "    with tf.compat.v1.variable_scope(\"create_model\"):\n",
    "        weights = tf.Variable(initial_value=tf.compat.v1.random_normal(shape=[1,1]),name=\"Weights\")\n",
    "        bias = tf.Variable(initial_value=tf.compat.v1.random_normal(shape=[1,1]),name=\"Bias\")\n",
    "        y_predict = tf.matmul(X,weights) + bias\n",
    "    \n",
    "    # 3.构造损失函数\n",
    "    with tf.compat.v1.variable_scope(\"loss_function\"):\n",
    "        error = tf.reduce_mean(tf.square(y_predict - y_true))\n",
    "    \n",
    "    # 4.优化损失\n",
    "    with tf.compat.v1.variable_scope(\"optimizer\"):\n",
    "        optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.05).minimize(error)\n",
    "    \n",
    "    # 2_收集变量\n",
    "    tf.compat.v1.summary.scalar(\"error\",error)\n",
    "    tf.compat.v1.summary.histogram(\"weights\",weights)\n",
    "    tf.compat.v1.summary.histogram(\"bias\",bias)\n",
    "    \n",
    "    # 3_合并变量\n",
    "    merged = tf.compat.v1.summary.merge_all()\n",
    "    \n",
    "    # 显式的初始化变量\n",
    "    init  = tf.compat.v1.global_variables_initializer()\n",
    "    \n",
    "    # 创建saver对象\n",
    "    saver = tf.compat.v1.train.Saver(max_to_keep=5)\n",
    "     \n",
    "    # 开启会话\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        # 初始化变量\n",
    "        sess.run(init)\n",
    "        \n",
    "        # 1_创建事件文件\n",
    "        file_writer = tf.compat.v1.summary.FileWriter(\"D:/AliyunEDU/04 summary2/\",graph=sess.graph)\n",
    "        \n",
    "        # 查看初始化模型参数后的值\n",
    "        print(\"训练前的模型参数为：权重：%f，偏置：%f，损失为：%f\" % (weights.eval(), bias.eval(), error.eval()))\n",
    "        \n",
    "        # 开始训练\n",
    "#         for i in range(200):\n",
    "#             sess.run(optimizer)\n",
    "#             print(\"第 %d 次训练后的模型参数为：权重：%f，偏置：%f，损失为：%f\" % (i+1,weights.eval(), bias.eval(), error.eval()))\n",
    "        \n",
    "#             # 4_运行合并变量操作\n",
    "#             summary = sess.run(merged)\n",
    "            \n",
    "#             # 5_将每次迭代后的变量写入事件文件\n",
    "#             file_writer.add_summary(summary,i)\n",
    "            \n",
    "#             # 保存模型\n",
    "#             if i % 10 == 0:\n",
    "#                 saver.save(sess,\"D:/AliyunEDU/04 model/lr_model.ckpt\")\n",
    "                \n",
    "    \n",
    "        if os.path.exists(\"D:/AliyunEDU/04 model/lr_model.ckpt\"):\n",
    "            saver.restore(sess,\"D:/AliyunEDU/04 model/checkpoint\")\n",
    "        print(\"训练后的模型训练参数为:权重%f，偏置%f，损失%f\" % (weights.eval(), bias.eval(), error.eval()))\n",
    "    \n",
    "    return None\n",
    "\n",
    "linear_regression_scope_save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.7.6 命令行参数的使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. tf.app.flags,他支持应用从命令行接受参数，狂热以用来指定集群配置等。在tf.app.flags下面有各种定义参数的类型。\n",
    "2. tf.app.flags有一个FLAGS标志，他在程序中可以调用我们前面具体定义的flag_name\n",
    "3. 通过tf.app.run()启动main(argv)函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
