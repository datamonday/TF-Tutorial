{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.06485754 -0.11709414  0.10522978  0.04428321]\n",
      " [ 0.06485754 -0.11709414  0.10522978  0.04428321]], shape=(2, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "class Linear(layers.Layer):\n",
    "    '''\n",
    "    Dense layer\n",
    "    '''\n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Linear, self).__init__()\n",
    "        # 初始化权重参数\n",
    "        w_init = tf.random_normal_initializer()\n",
    "        # 权重w占位符\n",
    "        self.w = tf.Variable(initial_value=w_init(shape=(input_dim, units),\n",
    "                                                 dtype='float32'),\n",
    "                            trainable=True)\n",
    "        # 初始化偏置参数\n",
    "        b_init = tf.zeros_initializer()\n",
    "        # 偏置b占位符\n",
    "        self.b = tf.Variable(initial_value=b_init(shape=(units,),\n",
    "                                                 dtype='float32'),\n",
    "                            trainable=True)\n",
    "    # 通过回调函数计算    \n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b # matmul 矩阵乘法\n",
    "        \n",
    "\n",
    "x = tf.ones((2,2))\n",
    "linear_layer = Linear(4,2)\n",
    "y = linear_layer(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert（断言）用于判断一个表达式，在表达式条件为 false 的时候触发异常。\n",
    "assert linear_layer.weights == [linear_layer.w, linear_layer.b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.04406109  0.09093019 -0.03715542  0.04563867]\n",
      " [-0.04406109  0.09093019 -0.03715542  0.04563867]], shape=(2, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "class Linear(layers.Layer):\n",
    "\n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Linear, self).__init__()\n",
    "        self.w = self.add_weight(shape=(input_dim, units),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(shape=(units,),\n",
    "                                 initializer='zeros',\n",
    "                                 trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "x = tf.ones((2, 2))\n",
    "linear_layer = Linear(4, 2)\n",
    "y = linear_layer(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2.]\n",
      "[4. 4.]\n"
     ]
    }
   ],
   "source": [
    "class ComputeSum(layers.Layer):\n",
    "\n",
    "    def __init__(self, input_dim):\n",
    "        super(ComputeSum, self).__init__()\n",
    "        self.total = tf.Variable(initial_value=tf.zeros((input_dim,)),\n",
    "                                 trainable=False)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # .assign_add(delta, use_locking=False, name=None, read_value=True) 属于 tf.Variable 的一个方法；\n",
    "        # 实现向变量delta(tensor)添加值。\n",
    "        self.total.assign_add(tf.reduce_sum(inputs, axis=0))\n",
    "        return self.total\n",
    "\n",
    "x = tf.ones((2, 2))\n",
    "my_sum = ComputeSum(2)\n",
    "y = my_sum(x)\n",
    "print(y.numpy())\n",
    "y = my_sum(x)\n",
    "print(y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: 1\n",
      "non-trainable weights: 1\n",
      "trainable_weights: []\n"
     ]
    }
   ],
   "source": [
    "print('weights:', len(my_sum.weights))\n",
    "print('non-trainable weights:', len(my_sum.non_trainable_weights))\n",
    "\n",
    "print('trainable_weights:', my_sum.trainable_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(layers.Layer):\n",
    "\n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Linear, self).__init__()\n",
    "        self.w = self.add_weight(shape=(input_dim, units),\n",
    "                                        initializer='random_normal',\n",
    "                                        trainable=True)\n",
    "        self.b = self.add_weight(shape=(units,),\n",
    "                                 initializer='zeros',\n",
    "                                 trainable=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(layers.Layer):\n",
    "\n",
    "    def __init__(self, units=32):\n",
    "        super(Linear, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        self.w = self.add_weight(shape=(input_shape[-1], self.units),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(shape=(self.units,),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在实例化时，不知道该调用什么输入\n",
    "linear_layer = Linear(32)\n",
    "# 首次调用该层时将动态创建该层的权重\n",
    "y = linear_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 32), dtype=float32, numpy=\n",
       "array([[ 0.16962165, -0.10605951, -0.0124007 ,  0.05497382, -0.04780749,\n",
       "        -0.00888127, -0.0770494 ,  0.21947056,  0.09034248, -0.05703586,\n",
       "        -0.0060977 , -0.07365507,  0.07659768, -0.01544504, -0.04743483,\n",
       "        -0.04726882,  0.10706575,  0.07933822,  0.00366602,  0.0726752 ,\n",
       "         0.09143378,  0.02786283, -0.01465205,  0.10379285,  0.17563567,\n",
       "         0.02788435, -0.05976434,  0.02714855,  0.08858202,  0.02146289,\n",
       "        -0.02282026,  0.05422783],\n",
       "       [ 0.16962165, -0.10605951, -0.0124007 ,  0.05497382, -0.04780749,\n",
       "        -0.00888127, -0.0770494 ,  0.21947056,  0.09034248, -0.05703586,\n",
       "        -0.0060977 , -0.07365507,  0.07659768, -0.01544504, -0.04743483,\n",
       "        -0.04726882,  0.10706575,  0.07933822,  0.00366602,  0.0726752 ,\n",
       "         0.09143378,  0.02786283, -0.01465205,  0.10379285,  0.17563567,\n",
       "         0.02788435, -0.05976434,  0.02714855,  0.08858202,  0.02146289,\n",
       "        -0.02282026,  0.05422783]], dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPBlock(layers.Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MLPBlock, self).__init__()\n",
    "        # 实例化子类层\n",
    "        self.linear_1 = Linear(32)\n",
    "        self.linear_2 = Linear(32)\n",
    "        self.linear_3 = Linear(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.linear_1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return self.linear_3(x)\n",
    "\n",
    "\n",
    "mlp = MLPBlock()\n",
    "# 首次调用mlp将创建权重\n",
    "y = mlp(tf.ones(shape=(3, 64))) \n",
    "print('weights:', len(mlp.weights))\n",
    "print('trainable weights:', len(mlp.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 动态变化正则化层\n",
    "class ActivityRegularizationLayer(layers.Layer):\n",
    "\n",
    "    def __init__(self, rate=1e-2):\n",
    "        super(ActivityRegularizationLayer, self).__init__()\n",
    "        self.rate = rate\n",
    "\n",
    "    def call(self, inputs):\n",
    "        self.add_loss(self.rate * tf.reduce_sum(inputs))\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OuterLayer(layers.Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(OuterLayer, self).__init__()\n",
    "        self.activity_reg = ActivityRegularizationLayer(1e-2)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.activity_reg(inputs)\n",
    "\n",
    "layer = OuterLayer()\n",
    "assert len(layer.losses) == 0  # 由于从未调用过该层，因此尚未计算任何损失\n",
    "\n",
    "_ = layer(tf.zeros(1, 1))\n",
    "assert len(layer.losses) == 1  # 通过实例化，产生了损失\n",
    "\n",
    "# 在每次__call__的开始时都会重置`layer.losses`\n",
    "_ = layer(tf.zeros(1, 1))\n",
    "assert len(layer.losses) == 1  # 这是调用时产生的损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(), dtype=float32, numpy=0.0020331577>]\n"
     ]
    }
   ],
   "source": [
    "class OuterLayer(layers.Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(OuterLayer, self).__init__()\n",
    "        self.dense = layers.Dense(32, kernel_regularizer=tf.keras.regularizers.l2(1e-3))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.dense(inputs)\n",
    "\n",
    "\n",
    "layer = OuterLayer()\n",
    "_ = layer(tf.zeros((1, 1)))\n",
    "\n",
    "# 这是由上面的`kernel_regularizer`创建的损失`1e-3 * sum（layer.dense.kernel ** 2）`，。\n",
    "print(layer.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-3aea24a43b52>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# 批处理遍历数据集\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mx_batch_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch_train\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# I实例化SGD优化器\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "# 定义损失\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# 批处理遍历数据集\n",
    "for x_batch_train, y_batch_train in train_dataset:\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = layer(x_batch_train)\n",
    "        # 批数据的损失\n",
    "        loss_value = loss_fn(y_batch_train, logits)\n",
    "        # 添加在此向前传递过程中创建的额外损失：\n",
    "        loss_value += sum(model.losses)\n",
    "\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'units': 64}\n"
     ]
    }
   ],
   "source": [
    "class Linear(layers.Layer):\n",
    "\n",
    "    def __init__(self, units=32):\n",
    "        super(Linear, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(shape=(input_shape[-1], self.units),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(shape=(self.units,),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'units': self.units}\n",
    "\n",
    "\n",
    "# 修改默认参数值，重新创建该层：\n",
    "layer = Linear(64)\n",
    "config = layer.get_config()\n",
    "print(config)\n",
    "new_layer = Linear.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDropout(layers.Layer):\n",
    "\n",
    "    def __init__(self, rate, **kwargs):\n",
    "        super(CustomDropout, self).__init__(**kwargs)\n",
    "        self.rate = rate\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        if training:\n",
    "            return tf.nn.dropout(inputs, rate=self.rate)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.block_1 = ResNetBlock()\n",
    "        self.block_2 = ResNetBlock()\n",
    "        self.global_pool = layers.GlobalAveragePooling2D()\n",
    "        self.classifier = Dense(num_classes)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.block_1(inputs)\n",
    "        x = self.block_2(x)\n",
    "        x = self.global_pool(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "resnet = ResNet()\n",
    "dataset = ...\n",
    "resnet.fit(dataset, epochs=10)\n",
    "resnet.save_weights(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    '''\n",
    "    将z重采样为（z_mean，z_log_var）向量，并用该向量编码一个数字。\n",
    "    '''\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "class Encoder(layers.Layer):\n",
    "    '''\n",
    "    将MNIST数字映射到三元组（z_mean，z_log_var，z）\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 latent_dim=32,\n",
    "                 intermediate_dim=64,\n",
    "                 name='encoder',\n",
    "                 **kwargs):\n",
    "        \n",
    "        super(Encoder, self).__init__(name=name, **kwargs)\n",
    "        self.dense_proj = layers.Dense(intermediate_dim, activation='relu')\n",
    "        self.dense_mean = layers.Dense(latent_dim)\n",
    "        self.dense_log_var = layers.Dense(latent_dim)\n",
    "        self.sampling = Sampling()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense_proj(inputs)\n",
    "        z_mean = self.dense_mean(x)\n",
    "        z_log_var = self.dense_log_var(x)\n",
    "        z = self.sampling((z_mean, z_log_var))\n",
    "        return z_mean, z_log_var, z\n",
    "\n",
    "\n",
    "class Decoder(layers.Layer):\n",
    "    '''\n",
    "    将编码的数字矢量z转换回可读的数字。\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 original_dim,\n",
    "                 intermediate_dim=64,\n",
    "                 name='decoder',\n",
    "                 **kwargs):\n",
    "        \n",
    "        super(Decoder, self).__init__(name=name, **kwargs)\n",
    "        self.dense_proj = layers.Dense(intermediate_dim, activation='relu')\n",
    "        self.dense_output = layers.Dense(original_dim, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense_proj(inputs)\n",
    "        return self.dense_output(x)\n",
    "\n",
    "\n",
    "class VariationalAutoEncoder(tf.keras.Model):\n",
    "    '''\n",
    "    将编码器和解码器组合为端到端模型以进行训练。\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 original_dim,\n",
    "                 intermediate_dim=64,\n",
    "                 latent_dim=32,\n",
    "                 name='autoencoder',\n",
    "                 **kwargs):\n",
    "        super(VariationalAutoEncoder, self).__init__(name=name, **kwargs)\n",
    "        self.original_dim = original_dim\n",
    "        self.encoder = Encoder(latent_dim=latent_dim,\n",
    "                               intermediate_dim=intermediate_dim)\n",
    "        self.decoder = Decoder(original_dim, intermediate_dim=intermediate_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        reconstructed = self.decoder(z)\n",
    "        # 添加KL散度正则化损失\n",
    "        kl_loss = - 0.5 * tf.reduce_mean(\n",
    "            z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n",
    "        self.add_loss(kl_loss)\n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n",
      "step 0: mean loss = tf.Tensor(0.2939159, shape=(), dtype=float32)\n",
      "step 100: mean loss = tf.Tensor(0.12394193, shape=(), dtype=float32)\n",
      "step 200: mean loss = tf.Tensor(0.09826896, shape=(), dtype=float32)\n",
      "step 300: mean loss = tf.Tensor(0.088605046, shape=(), dtype=float32)\n",
      "step 400: mean loss = tf.Tensor(0.08379695, shape=(), dtype=float32)\n",
      "step 500: mean loss = tf.Tensor(0.080534376, shape=(), dtype=float32)\n",
      "step 600: mean loss = tf.Tensor(0.07844055, shape=(), dtype=float32)\n",
      "step 700: mean loss = tf.Tensor(0.07689294, shape=(), dtype=float32)\n",
      "step 800: mean loss = tf.Tensor(0.07575489, shape=(), dtype=float32)\n",
      "step 900: mean loss = tf.Tensor(0.07475837, shape=(), dtype=float32)\n",
      "Start of epoch 1\n",
      "step 0: mean loss = tf.Tensor(0.07446379, shape=(), dtype=float32)\n",
      "step 100: mean loss = tf.Tensor(0.073832676, shape=(), dtype=float32)\n",
      "step 200: mean loss = tf.Tensor(0.07333698, shape=(), dtype=float32)\n",
      "step 300: mean loss = tf.Tensor(0.07288531, shape=(), dtype=float32)\n",
      "step 400: mean loss = tf.Tensor(0.07256026, shape=(), dtype=float32)\n",
      "step 500: mean loss = tf.Tensor(0.072173476, shape=(), dtype=float32)\n",
      "step 600: mean loss = tf.Tensor(0.07188245, shape=(), dtype=float32)\n",
      "step 700: mean loss = tf.Tensor(0.07159958, shape=(), dtype=float32)\n",
      "step 800: mean loss = tf.Tensor(0.07137791, shape=(), dtype=float32)\n",
      "step 900: mean loss = tf.Tensor(0.07111288, shape=(), dtype=float32)\n",
      "Start of epoch 2\n",
      "step 0: mean loss = tf.Tensor(0.07103922, shape=(), dtype=float32)\n",
      "step 100: mean loss = tf.Tensor(0.07087795, shape=(), dtype=float32)\n",
      "step 200: mean loss = tf.Tensor(0.07075036, shape=(), dtype=float32)\n",
      "step 300: mean loss = tf.Tensor(0.07059481, shape=(), dtype=float32)\n",
      "step 400: mean loss = tf.Tensor(0.07050161, shape=(), dtype=float32)\n",
      "step 500: mean loss = tf.Tensor(0.070355415, shape=(), dtype=float32)\n",
      "step 600: mean loss = tf.Tensor(0.070239566, shape=(), dtype=float32)\n",
      "step 700: mean loss = tf.Tensor(0.07012396, shape=(), dtype=float32)\n",
      "step 800: mean loss = tf.Tensor(0.070030354, shape=(), dtype=float32)\n",
      "step 900: mean loss = tf.Tensor(0.069898024, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "original_dim = 784\n",
    "vae = VariationalAutoEncoder(original_dim, 64, 32)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "mse_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "loss_metric = tf.keras.metrics.Mean()\n",
    "\n",
    "(x_train, _), _ = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(60000, 784).astype('float32') / 255\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Start of epoch %d' % (epoch,))\n",
    "    for step, x_batch_train in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            reconstructed = vae(x_batch_train)\n",
    "            # 计算重构损失\n",
    "            loss = mse_loss_fn(x_batch_train, reconstructed)\n",
    "            loss += sum(vae.losses)  # 添加KL散度正则化损失\n",
    "\n",
    "        grads = tape.gradient(loss, vae.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, vae.trainable_weights))\n",
    "\n",
    "        loss_metric(loss)\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print('step %s: mean loss = %s' % (step, loss_metric.result()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 6s 92us/sample - loss: 0.0747\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.0677\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.0675\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e26e7e6888>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae = VariationalAutoEncoder(784, 64, 32)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "vae.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())\n",
    "vae.fit(x_train, x_train, epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 0.0748\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 4s 74us/sample - loss: 0.0676\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 4s 73us/sample - loss: 0.0676\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e266d010c8>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_dim = 784\n",
    "intermediate_dim = 64\n",
    "latent_dim = 32\n",
    "\n",
    "# 定义编码器模型\n",
    "original_inputs = tf.keras.Input(shape=(original_dim,), name='encoder_input')\n",
    "x = layers.Dense(intermediate_dim, activation='relu')(original_inputs)\n",
    "z_mean = layers.Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = layers.Dense(latent_dim, name='z_log_var')(x)\n",
    "z = Sampling()((z_mean, z_log_var))\n",
    "encoder = tf.keras.Model(inputs=original_inputs, outputs=z, name='encoder')\n",
    "\n",
    "# 定义解码器模型\n",
    "latent_inputs = tf.keras.Input(shape=(latent_dim,), name='z_sampling')\n",
    "x = layers.Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
    "outputs = layers.Dense(original_dim, activation='sigmoid')(x)\n",
    "decoder = tf.keras.Model(inputs=latent_inputs, outputs=outputs, name='decoder')\n",
    "\n",
    "# 定义 VAE 模型\n",
    "outputs = decoder(z)\n",
    "vae = tf.keras.Model(inputs=original_inputs, outputs=outputs, name='vae')\n",
    "\n",
    "# 添加KL散度正则化损失\n",
    "kl_loss = - 0.5 * tf.reduce_mean(\n",
    "    z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n",
    "vae.add_loss(kl_loss)\n",
    "\n",
    "# 训练\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "vae.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())\n",
    "vae.fit(x_train, x_train, epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
